require(ggplot2)
install.packages("ggplot2")
library("ggplot2")
require(ggplot2)
ggplot(tw.df)+geom_point(aes(x=created,y=screenName))
require(twitteR)
username='tomgara'
#the most tweets we can bring back from a user timeline is the most recent 3600...
mht=userTimeline(snarwani,n=3200)
tw.df=twListToDF(mht)
require(twitteR)
username='tomgara'
#the most tweets we can bring back from a user timeline is the most recent 3600...
mht=userTimeline(snarwani,n=3200)
tw.df=twListToDF(mht)
require(twitteR)
username='tomgara'
#the most tweets we can bring back from a user timeline is the most recent 3600...
mht=userTimeline(snarwani,n=3200)
tw.df=twListToDF(mht)
require(twitteR)
username='tomgara'
#the most tweets we can bring back from a user timeline is the most recent 3600...
mht=userTimeline(snarwani,n=3200)
tw.df=twListToDF(mht)
require(twitteR)
username='tomgara'
#the most tweets we can bring back from a user timeline is the most recent 3600...
mht=userTimeline(snarwani,n=3200)
tw.df=twListToDF(mht)
require(twitteR)
username='tomgara'
#the most tweets we can bring back from a user timeline is the most recent 3600...
mht=userTimeline(tomgara,n=3200)
tw.df=twListToDF(mht)
#As I've done in previous scripts, pull out the names of folk who have been "old-fashioned RTd"...
require(stringr)
trim <- function (x) sub('@','',x)
tw.df$rt=sapply(tw.df$text,function(tweet) trim(str_match(tweet,"^RT (@[[:alnum:]_]*)")[2]))
tw.df$rtt=sapply(tw.df$rt,function(rt) if (is.na(rt)) 'T' else 'RT')
require(ggplot2)
ggplot(tw.df)+geom_point(aes(x=created,y=screenName))
require(twitteR)
username='tomgara'
#the most tweets we can bring back from a user timeline is the most recent 3600...
mht=userTimeline(username,n=3200)
require(twitteR)
username='tomgara'
#the most tweets we can bring back from a user timeline is the most recent 3600...
mht=userTimeline(username,n=3200)
source("twitter_extraction.R")
require(twitteR)
#Pull in a search around a hashtag.
searchTerm='#ukgc12'
rdmTweets <- searchTwitter(searchTerm, n=500)
# Note that the Twitter search API only goes back 1500 tweets
#Plot of tweet behaviour by user over time
#Based on @mediaczar's http://blog.magicbeanlab.com/networkanalysis/how-should-page-admins-deal-with-flame-wars/
#Make use of a handy dataframe creating twitteR helper function
tw.df=twListToDF(rdmTweets)
#@mediaczar's plot uses a list of users ordered by accession to user list
## 1) find earliest tweet in searchlist for each user [ http://stackoverflow.com/a/4189904/454773 ]
require(plyr)
tw.dfx=ddply(tw.df, .var = "screenName", .fun = function(x) {return(subset(x, created %in% min(created),select=c(screenName,created)))})
## 2) arrange the users in accession order
tw.dfxa=arrange(tw.dfx,-desc(created))
## 3) Use the username accession order to order the screenName factors in the searchlist
tw.df$screenName=factor(tw.df$screenName, levels = tw.dfxa$screenName)
#ggplot seems to be able to cope with time typed values...
require(ggplot2)
ggplot(tw.df)+geom_point(aes(x=created,y=screenName))
require(twitteR)
#Pull in a search around a hashtag.
searchTerm='#SOPA'
rdmTweets <- searchTwitter(searchTerm, n=3200)
require(twitteR)
#Pull in a search around a hashtag.
searchTerm='#SOPA'
rdmTweets <- searchTwitter(searchTerm, n=3200)
require(twitteR)
#Pull in a search around a hashtag.
searchTerm='#SOPA'
rdmTweets <- searchTwitter(searchTerm, n=3200)
# Note that the Twitter search API only goes back 1500 tweets
#Plot of tweet behaviour by user over time
#Based on @mediaczar's http://blog.magicbeanlab.com/networkanalysis/how-should-page-admins-deal-with-flame-wars/
#Make use of a handy dataframe creating twitteR helper function
tw.df=twListToDF(rdmTweets)
#@mediaczar's plot uses a list of users ordered by accession to user list
## 1) find earliest tweet in searchlist for each user [ http://stackoverflow.com/a/4189904/454773 ]
require(plyr)
tw.dfx=ddply(tw.df, .var = "screenName", .fun = function(x) {return(subset(x, created %in% min(created),select=c(screenName,created)))})
## 2) arrange the users in accession order
tw.dfxa=arrange(tw.dfx,-desc(created))
## 3) Use the username accession order to order the screenName factors in the searchlist
tw.df$screenName=factor(tw.df$screenName, levels = tw.dfxa$screenName)
#ggplot seems to be able to cope with time typed values...
require(ggplot2)
ggplot(tw.df)+geom_point(aes(x=created,y=screenName))
require(twitteR)
#Pull in a search around a hashtag.
searchTerm='#SOPA'
rdmTweets <- searchTwitter(searchTerm, n=500)
# Note that the Twitter search API only goes back 1500 tweets
#Plot of tweet behaviour by user over time
#Based on @mediaczar's http://blog.magicbeanlab.com/networkanalysis/how-should-page-admins-deal-with-flame-wars/
#Make use of a handy dataframe creating twitteR helper function
tw.df=twListToDF(rdmTweets)
#@mediaczar's plot uses a list of users ordered by accession to user list
## 1) find earliest tweet in searchlist for each user [ http://stackoverflow.com/a/4189904/454773 ]
require(plyr)
tw.dfx=ddply(tw.df, .var = "screenName", .fun = function(x) {return(subset(x, created %in% min(created),select=c(screenName,created)))})
## 2) arrange the users in accession order
tw.dfxa=arrange(tw.dfx,-desc(created))
## 3) Use the username accession order to order the screenName factors in the searchlist
tw.df$screenName=factor(tw.df$screenName, levels = tw.dfxa$screenName)
#ggplot seems to be able to cope with time typed values...
require(ggplot2)
ggplot(tw.df)+geom_point(aes(x=created,y=screenName))
update.packages()
installed.packages()
help(package="twitteR")
lm(mpg~wt, data=mtcars)
lmfit <- lm(mpg~wt, data=mtcars)
lmfit
mode(lmfit)
plot(lmfit)
summary(lmfit)
cook<-cooks.distance(lmfit)
plot(cook)
?plot(cook)
?cook
?cooks.distancw
?cooks.distance
predict(lmfit, mynewdata)
help(lm)
??cook
??cooks.distance
?influence.measures
a <- 1:20
dim(a) <- c(4,5)
a
a[2,4]
a <- array(1:20), dim=c(4,5)
a <- array(1:20), dim=c(4,5))
a <- array(1:20, dim=c(4,5))
a
a <- array(0, dim=c(4,5))
a
i <- array(c(1,2,1,2), dim=c(2,2))
i
a[i]
attach(mtcars)
plot(wt, mpg)
abline(lm(mpg~wt))
title("Regression of MPG on Weight")
detach(mtcars)
detach(mtcars)
attach(mtcars)
n <- 10
mycolors <- rainbow(n)
pie(rep(1, n), labels=mycolors, col=mycolors)
mygrays <- gray(0:n/n)
pie(rep(1, n), labels=mygrays, col=mygrays)
n <- 10
mycolors <- rainbow(n)
pie(rep(1, n), labels=mycolors, col=mycolors)
mygrays <- gray(0:n/n)
pie(rep(1, n), labels=mygrays, col=mygrays)
a[2,4]
a
A = array(c(3,2,-1,2,-2,0.5,-1,4,-1), dim(3,3))
A = array(c(3,2,-1,2,-2,0.5,-1,4,-1), dim c(3,3))
A = array(c(3,2,-1,2,-2,0.5,-1,4,-1), dim, c(3,3))
A = array(c(3,2,-1,2,-2,0.5,-1,4,-1), dim=c(3,3))
b = c(1,-2,0)
b
solve(A,b)
A
A %*% solve(A,b)
solve(a)
solve(A)
solve(A) %*% b
solve(A) %*% c(1,-2,0)
# Download file and parse it
appid<-'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ8M-'
street<-"11408 Bellflower Road"
RCurl<-paste(
"http://local.yahooapis.com/MapsService/V1/geocode?appid=",
appid,
"&street=",
URLencode(street),
"&city=Cleveland&state=OH"
,sep="")
#xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE)
xmlResult<-xmlTreeParse(getURL(RCurl))
# Install and load the XML package
install.packages("XML")
library("XML")
# Download file and parse it
appid<-'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ8M-'
street<-"11408 Bellflower Road"
RCurl<-paste(
"http://local.yahooapis.com/MapsService/V1/geocode?appid=",
appid,
"&street=",
URLencode(street),
"&city=Cleveland&state=OH"
,sep="")
#xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE)
xmlResult<-xmlTreeParse(getURL(RCurl))
str(xmlResult)
library("RCurl")
# Download file and parse it
appid<-'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ8M-'
street<-"11408 Bellflower Road"
RCurl<-paste(
"http://local.yahooapis.com/MapsService/V1/geocode?appid=",
appid,
"&street=",
URLencode(street),
"&city=Cleveland&state=OH"
,sep="")
#xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE)
xmlResult<-xmlTreeParse(getURL(RCurl))
#Print the output
str(xmlResult)
traceback()
?traceback()
options(error = recover)
# Install and load RCurl
install.packages("RCurl")
library("RCurl")
# Install and load the XML package
install.packages("XML")
library("XML")
# Download file and parse it
appid<-'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ8M-'
street<-"11408 Bellflower Road"
RCurl<-paste(
"http://local.yahooapis.com/MapsService/V1/geocode?appid=",
appid,
"&street=",
URLencode(street),
"&city=Cleveland&state=OH"
,sep="")
#xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE)
xmlResult<-xmlTreeParse(getURL(RCurl))
#Print the output
str(xmlResult)
# Download file and parse it
appid<-'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ8M-'
street<-"11408 Bellflower Road"
RCurl<-paste("http://local.yahooapis.com/MapsService/V1/geocode?appid=", appid, "&street=", URLencode(street), "&city=Cleveland&state=OH", sep="")
#xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE)
xmlResult<-xmlTreeParse(getURL(RCurl))
str(xmlResult)
2
str(xmlResult)
appid<-'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ8M-'
street<-"11408 Bellflower Road"
RCurl<-paste("http://local.yahooapis.com/MapsService/V1/geocode?appid=", appid, "&street=", URLencode(street), "&city=Cleveland&state=OH", sep="")
#xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE)
xmlResult<-xmlTreeParse(getURL(RCurl))
#Print the output
str(xmlResult)
library("RCurl")
library("XML")
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("11408+Bellflower+Road", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
doc <- xmlToList(xmlTreeParse(urlRequest)) # Convert to List
str(doc)
str(lat)
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("11408+Bellflower+Road", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
doc <- xmlResult(xmlTreeParse(urlRequest)) # Convert to List
str(doc)
str(lat)
# Download file and parse it
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("11408+Bellflower+Road", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
xmlResult<-xmlTreeParse(getURL(RCurl))
#Print the output
str(xmlResult)
str(xmlResult)
xmlResult<-xmlTreeParse(getURL(RCurl))
str(latitude)
# Download file and parse it
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("11408+Bellflower+Road", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
doc <- xmlToList(xmlTreeParse(urlRequest)) # Convert to List
str(doc)
#Print the output
str(doc
str(doc)
str(doc)
str(latitude)
str(latitude$doc)
xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE)
# Download file and parse it
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("11408+Bellflower+Road", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE)
str(doc)
str(doc$latitude)
str(latitude)
latitutude$doc
latitude$doc
ls.str(doc)
doc
doc$latitude
doc$Latitude
library("XML")
library("RCurl")
# Download file and parse it
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("11408+Bellflower+Road", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
doc <- xmlToList(xmlTreeParse(urlRequest)) # Convert to List
# Download file and parse it
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("11408+Bellflower+Road", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
xmlResult <- xmlToList(xmlTreeParse(urlRequest)) # Convert to List
ls.str(xmlResult)
lat<-xmlResult[['doc']][['ResultSet']][['Result']][['Latitude']][['text']]
lat
lat<-xmlResult[latitude]
lat<-xmlResult[$latitude]
lat<-xmlResult[latitude]
lat<-xmlResult['latitude']
lat
lat<-xmlResult['$latitude']
lat
xmlResult
lat<-xmlResult['$Result$latitude']
lat
lat<-xmlResult['Result$latitude']
lat
lat<-xmlResult[Result$latitude]
lat<-xmlResult[$Result$latitude]
lat<-xmlResult$Latitude
lat
lat<-xmlResult$Result$Latitude
lat
lat<-xmlResult[$Result$Latitude]
lat<-xmlResult[Result$Latitude]
lat<-xmlResult[Latitude]
lat<-xmlResult["Latitude"]
lat
lat<-xmlResult["$Result$Latitude"]
lat
xmlResult
$Result$latitude
Result$latitude
xmlResult$Result$latitude
lat<-xmlResult$Result$latitude
lat
lat<-xmlResult$Result$latitude
long<-xmlResult$Result$longitude
lat
long
str(lat)
lat<-xmlResult$Locale
lat
xmlResult
# Download file and parse it
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("1+Fake+Street", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
xmlResult <- xmlToList(xmlTreeParse(urlRequest)) # Convert to List
ls.str(xmlResult)
# Download file and parse it
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("1+Fake+Street", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
xmlResult <- xmlToList(xmlTreeParse(urlRequest)) # Convert to List
ls.str(xmlResult)
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("1+Fake+Street", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
xmlResult <- xmlToList(xmlTreeParse(urlRequest)) # Convert to List
ls.str(xmlResult)
library("XML")
library("RCurl")
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("1+Fake+Street", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
xmlResult <- xmlToList(xmlTreeParse(urlRequest)) # Convert to List
ls.str(xmlResult)
# Store the latitude and longitude in new objects
xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE,addAttributeNamespaces=TRUE)
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("1+Fake+Street", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
xmlResult <- xmlToList(xmlTreeParse(urlRequest)) # Convert to List
# Download file and parse it
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("1+Fake+Street", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
# xmlResult <- xmlToList(xmlTreeParse(urlRequest)) # Convert to List
xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE,addAttributeNamespaces=TRUE)
ls.str(xmlResult)
# Download file and parse it
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
address    <- paste("1+Fake+Street", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
# xmlResult <- xmlToList(xmlTreeParse(urlRequest)) # Convert to List
xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE,addAttributeNamespaces=TRUE)
xmlResult<-xmlTreeParse(urlRequest,isURL=TRUE,addAttributeNamespaces=TRUE)
ls.str(xmlResult)
str(xmlResult)
xmlResult$doc$children$ResultSet$children$Result$attributes['precision']
xmlResult
xmlResult<-xmlTree(urlRequest,isURL=TRUE,addAttributeNamespaces=TRUE)
xmlResult<-xmlToList(urlRequest,isURL=TRUE,addAttributeNamespaces=TRUE)
xmlResult<-xmlTreeParse(urlRequest,isURL=TRUE,addAttributeNamespaces=TRUE)
str(xmlResult)
xmlResult$doc$children$ResultSet$children$Result$attributes['precision']
str(xmlResult)
precisio
precision
# Download file and parse it
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
street<-"1 Fake St"
requestUrl<-paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE,addAttributeNamespaces=TRUE)
xmlResult$doc$children$ResultSet$children$Result$attributes['precision']
str.XmlResult
str(XmlResult)
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
street<-"1 Fake St"
requestUrl<-paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE,addAttributeNamespaces=TRUE)
str(XmlResult)
appid      <- 'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ    8M-'
street<-"1 Fake St"
requestUrl<-paste("http://where.yahooapis.com/geocode?q=", address, appid=appid, sep = "")
xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE,addAttributeNamespaces=TRUE)
str(XmlResult)
appid<-'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ8M-'
street<-"1 Fake St"
requestUrl<-paste("http://where.yahooapis.com/geocode?q=", address, appid=appid, sep = "")
xmlResult<-xmlTreeParse(requestUrl,isURL=TRUE,addAttributeNamespaces=TRUE)
str(XmlResult)
xmlResult$doc$children$ResultSet$children$Result$attributes['precision']
appid<-'ucVVQzLV34GQR4ppLwbdW6G8cCSZDoCBqAc53NXsWB3gXkmP1I4epLwMxboV.PfADi_2ubr2A7Cg8FO4Z3xVxxujza2FJ8M-'
address    <- paste("1+Fake+Street", "Cleveland", "OH", sep=",+")
urlRequest <- paste("http://where.yahooapis.com/geocode?q=",
address, appid=appid, sep = "")
# xmlResult <- xmlToList(xmlTreeParse(urlRequest)) # Convert to List
xmlResult<-xmlTreeParse(urlRequest,isURL=TRUE,addAttributeNamespaces=TRUE)
str(xmlResult)
?urlRequest
??urlRequest
?rCurl
??rCurl
#Load the TraMineR and cluster libraries
library(TraMineR)
library(cluster)
#Load the dataset
github <- read.csv(file = "GitSequences.csv", header = TRUE)
#Display summary statistics
summary(github)
#Convert to compressed format
github.comp <- seqconc(github)
# Display summary
summary(github.comp)
# Create a sequence object
github.seq<-seqdef(github)
#Display summary
summary(github.seq)
#Define costs and create a distance matrix
costs <- seqsubm(github.seq, method="TRATE")
github.om <- seqdist(github.seq, method="OM", indel=1, sm=costs)
# Display the output in rounded format
round(github.om)
lusterward <- agnes(github.om, diss = TRUE, method = "ward")
github <- read.csv(file = "GitSequences.csv", header = TRUE)
setwd("~/github/local/VOSS-Sequencing-Toolkit")
github <- read.csv(file = "GitSequences.csv", header = TRUE)
summary(github)
github.comp <- seqconc(github)
summary(github.comp)
github.seq<-seqdef(github)
summary(github.seq)
costs <- seqsubm(github.seq, method="TRATE")
github.om <- seqdist(github.seq, method="OM", indel=1, sm=costs)
round(github.om)
lusterward <- agnes(github.om, diss = TRUE, method = "ward")
plot(lusterward, which.plots = 2)
